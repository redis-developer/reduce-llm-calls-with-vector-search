{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Semantic Caching with LangCache\n",
    "\n",
    "Semantic caching is an intelligent caching strategy that stores and retrieves responses based on the meaning of queries rather than exact text matches. Unlike traditional caching that requires identical strings, semantic caching can return cached responses for questions that are semantically similar, even when phrased differently.\n",
    "\n",
    "## Semantic Caching vs. Traditional Caching vs. LLM Re-generation\n",
    "\n",
    "**Traditional caching** stores responses using exact query strings as keys:\n",
    "- **Fast retrieval** for identical queries\n",
    "- **Cache misses** for any variation in phrasing, even minor differences\n",
    "- **Low cache hit rates** in conversational applications where users rarely phrase questions identically\n",
    "\n",
    "**LLM re-generation** involves calling the language model for every query:\n",
    "- **Flexible** handling of any question variation\n",
    "- **High API costs** and latency for repeated similar questions\n",
    "\n",
    "**Semantic caching** uses vector similarity to match queries with cached responses:\n",
    "- **High cache hit rates** by matching semantically similar questions\n",
    "- **Cost reduction** by avoiding redundant LLM calls for similar queries\n",
    "- **Fast retrieval** through vector similarity search\n",
    "\n",
    "In this notebook, we'll implement semantic caching using RedisVL with pre-generated FAQs about a Chevrolet Colorado vehicle brochure, demonstrating how semantic similarity can dramatically improve cache hit rates compared to exact string matching."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "This semantic caching implementation with LangCache requires one Python library that integrates with LangCache managed service. LangCache managed service, in turn, will be responsible for generating the embeddings and storing & retrieving entries from the cache."
   ],
   "id": "1b93feba158c4620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:56.058770Z",
     "start_time": "2025-10-14T18:06:55.386840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "%pip install langcache"
   ],
   "id": "86fec7b4712989b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langcache in ./.venv/lib/python3.13/site-packages (0.10.1)\r\n",
      "Requirement already satisfied: httpcore>=1.0.9 in ./.venv/lib/python3.13/site-packages (from langcache) (1.0.9)\r\n",
      "Requirement already satisfied: httpx>=0.28.1 in ./.venv/lib/python3.13/site-packages (from langcache) (0.28.1)\r\n",
      "Requirement already satisfied: pydantic>=2.11.2 in ./.venv/lib/python3.13/site-packages (from langcache) (2.11.9)\r\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpcore>=1.0.9->langcache) (2025.8.3)\r\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore>=1.0.9->langcache) (0.16.0)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx>=0.28.1->langcache) (4.11.0)\r\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx>=0.28.1->langcache) (3.10)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.11.2->langcache) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.11.2->langcache) (2.33.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.11.2->langcache) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.11.2->langcache) (0.4.1)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx>=0.28.1->langcache) (1.3.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuring LangCache SDK\n",
    "\n",
    "In order to connect to LangCache, we will need to configure its client with server URL, the Cache ID, and the API key. All of those can be found in the configuration of LangCache in https://cloud.redis.io"
   ],
   "id": "2f0f07b33554953a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:07:47.531546Z",
     "start_time": "2025-10-14T18:07:47.514774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langcache import LangCache\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"LANG_CACHE_API_KEY\")\n",
    "\n",
    "lang_cache = LangCache(\n",
    "    server_url=\"https://gcp-us-east4.langcache.redis.io\",\n",
    "    cache_id=\"30b5d6f3fafb40d69b18b255e0a7b3c3\",\n",
    "    api_key=api_key,\n",
    ")"
   ],
   "id": "775c7c97cfd27e0b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding an entry to the cache\n",
    "\n",
    "Adding an entry to the cache is as easy as calling a single function that receives the prompt and the response as parameters."
   ],
   "id": "68e8d80fa8670536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:07:52.815537Z",
     "start_time": "2025-10-14T18:07:52.273186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_response = lang_cache.set(\n",
    "    prompt=\"How does semantic caching work?\",\n",
    "    response=\"Semantic caching stores and retrieves data based on meaning, not exact matches.\"\n",
    ")\n",
    "print(\"Save entry response:\", save_response)"
   ],
   "id": "23ac7bce496dcb4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save entry response: entry_id='fda1b671e21b06a0a957c04b1692ab90'\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieving entries from the cache\n",
    "\n",
    "Retrieving entries is equally as simple. We just need to invoke the search function passing the prompt as parameter."
   ],
   "id": "9ceb405b3af8b31a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:07:54.659046Z",
     "start_time": "2025-10-14T18:07:54.346371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_response = lang_cache.search(\n",
    "    prompt=\"What is semantic caching?\"\n",
    ")\n",
    "print(\"Search entry response:\", search_response)"
   ],
   "id": "8d6c1f1370bf8426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search entry response: data=[CacheEntry(id='13a51eaa233436e3b6910fe1542f0c22', prompt='What is semantic caching?', response='Semantic caching stores and retrieves data based on meaning, not exact matches.', attributes={}, similarity=1.0, search_strategy=<SearchStrategy.SEMANTIC: 'semantic'>), CacheEntry(id='fda1b671e21b06a0a957c04b1692ab90', prompt='How does semantic caching work?', response='Semantic caching stores and retrieves data based on meaning, not exact matches.', attributes={}, similarity=0.9291304, search_strategy=<SearchStrategy.SEMANTIC: 'semantic'>)]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When using the langcache embedding model trained by Redis, we can also avoid false negatives. In this case, we cached \"What is semantic caching?\", but now we're checking in the cache for the opposite by just adding a new word: 'not' - Most embedding models would struggle with this and find both sentences to still be very similar. But Redis' trained model excels at such cases.",
   "id": "a10f02b4593f338c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:07:59.450410Z",
     "start_time": "2025-10-14T18:07:59.171498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_response = lang_cache.search(\n",
    "    prompt=\"What is not semantic caching?\"\n",
    ")\n",
    "print(\"Search entry response:\", search_response)"
   ],
   "id": "96235b892b97bb4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search entry response: data=[]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we can also check something that hasn't been cached at all, expecting nothing to be returned.'",
   "id": "d4ad1683c9b7597b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:08:02.829426Z",
     "start_time": "2025-10-14T18:08:02.519019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_response = lang_cache.search(\n",
    "    prompt=\"What's the fastest commercial plane available?\"\n",
    ")\n",
    "print(\"Search entry response:\", search_response)"
   ],
   "id": "4c6393a88396b6c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search entry response: data=[]\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
