{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Semantic Caching\n",
    "\n",
    "Semantic caching is an intelligent caching strategy that stores and retrieves responses based on the meaning of queries rather than exact text matches. Unlike traditional caching that requires identical strings, semantic caching can return cached responses for questions that are semantically similar, even when phrased differently.\n",
    "\n",
    "## Semantic Caching vs. Traditional Caching vs. LLM Re-generation\n",
    "\n",
    "**Traditional caching** stores responses using exact query strings as keys:\n",
    "- **Fast retrieval** for identical queries\n",
    "- **Cache misses** for any variation in phrasing, even minor differences\n",
    "- **Low cache hit rates** in conversational applications where users rarely phrase questions identically\n",
    "\n",
    "**LLM re-generation** involves calling the language model for every query:\n",
    "- **Flexible** handling of any question variation\n",
    "- **High API costs** and latency for repeated similar questions\n",
    "\n",
    "**Semantic caching** uses vector similarity to match queries with cached responses:\n",
    "- **High cache hit rates** by matching semantically similar questions\n",
    "- **Cost reduction** by avoiding redundant LLM calls for similar queries\n",
    "- **Fast retrieval** through vector similarity search\n",
    "\n",
    "In this notebook, we'll implement semantic caching using RedisVL with pre-generated FAQs about a Chevrolet Colorado vehicle brochure, demonstrating how semantic similarity can dramatically improve cache hit rates compared to exact string matching."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "This semantic caching implementation requires several Python libraries that work together to provide vector embeddings, caching functionality, and LLM integration.\n",
    "\n",
    "- RedisVL - Provides the semantic caching functionality built on top of Redis. This library handles vector storage, similarity search, and the caching interface we'll use to store and retrieve semantically similar queries.\n",
    "- Sentence Transformers - Supplies pre-trained models for converting text into high-quality vector embeddings. These embeddings capture semantic meaning, allowing us to find similar queries even when they're phrased differently."
   ],
   "id": "1b93feba158c4620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:20.098377Z",
     "start_time": "2025-10-14T18:06:19.430164Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install -q \"redisvl>=0.8.2\" sentence-transformers",
   "id": "86fec7b4712989b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading Pre-Generated FAQs\n",
    "\n",
    "For this semantic caching demonstration, we'll use pre-generated frequently asked questions (FAQs) about a Chevrolet Colorado vehicle brochure. These FAQs were created by processing the vehicle documentation and extracting question-answer pairs using an LLM.\n"
   ],
   "id": "e165ff9cc99f99a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:27.394670Z",
     "start_time": "2025-10-14T18:06:27.389670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Read the saved FAQs\n",
    "with open('../data/3_colorado_faqs.json', 'r', encoding='utf-8') as f:\n",
    "    all_faqs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_faqs)} FAQs from file\")"
   ],
   "id": "8ec9e1d079d67d99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 346 FAQs from file\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting up the Text Vectorizer\n",
    "\n",
    "The vectorizer is responsible for converting text into numerical vector representations that capture semantic meaning. RedisVL provides several vectorizer options such as OpenAI and VertexAI. We're using the HuggingFace Text Vectorizer for this example."
   ],
   "id": "d85d82667d13959a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:35.353238Z",
     "start_time": "2025-10-14T18:06:29.229291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from redisvl.utils.vectorize import HFTextVectorizer\n",
    "\n",
    "vectorizer = HFTextVectorizer(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ],
   "id": "41442d387fd94410",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raphaeldelio/reduce-llm-calls-with-vector-search/python/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:06:30 numexpr.utils INFO   NumExpr defaulting to 11 threads.\n",
      "20:06:32 sentence_transformers.SentenceTransformer INFO   Use pytorch device_name: mps\n",
      "20:06:32 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vectorizing the FAQ record pairs",
   "id": "80d84cca67658869"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:37.458703Z",
     "start_time": "2025-10-14T18:06:36.910442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embed each chunk content using the vectorizer\n",
    "embeddings = vectorizer.embed_many([pair[\"prompt\"] for pair in all_faqs])\n",
    "\n",
    "# Check to make sure we've created enough embeddings, 1 per FAQ record\n",
    "len(embeddings) == len(all_faqs)"
   ],
   "id": "8916b2a4ebf5fda9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating the SemanticCache",
   "id": "cfc0dd74e0739bd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:45.606917Z",
     "start_time": "2025-10-14T18:06:45.598293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "\n",
    "cache = SemanticCache(vectorizer=vectorizer, distance_threshold=0.2, overwrite=True)"
   ],
   "id": "31621ead9f95beea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:06:45 redisvl.index.index INFO   Index already exists, overwriting.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adding the previously vectorized FAQ pairs to the semantic cache",
   "id": "a232a1e1e0c44efc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:47.318836Z",
     "start_time": "2025-10-14T18:06:47.193291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, entry in enumerate(all_faqs):\n",
    "    cache.store(prompt=entry[\"prompt\"], response=entry[\"response\"], vector=embeddings[i])"
   ],
   "id": "95bff8ff3fa6e2dd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing the Semantic Cache",
   "id": "43e7fa5897cf3f14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:48.671867Z",
     "start_time": "2025-10-14T18:06:48.589932Z"
    }
   },
   "cell_type": "code",
   "source": "cache.check(\"What models of chevy colorado are available?\")",
   "id": "8e2e77ce47b2ed33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entry_id': '93f888c0c7ff6f0852dd10581cfe0851d728e785a2a5daeef2b426f86f45dc28',\n",
       "  'prompt': 'What are the available models of the Colorado?',\n",
       "  'response': 'The available models of the Colorado are WT, LT, Z71, and ZR2.',\n",
       "  'vector_distance': 0.18787831068,\n",
       "  'inserted_at': 1760465207.2,\n",
       "  'updated_at': 1760465207.2,\n",
       "  'key': 'llmcache:93f888c0c7ff6f0852dd10581cfe0851d728e785a2a5daeef2b426f86f45dc28'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:49.794730Z",
     "start_time": "2025-10-14T18:06:49.778319Z"
    }
   },
   "cell_type": "code",
   "source": "cache.check(\"What entertainment system comes with the car?\")",
   "id": "9d3c460928245325",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entry_id': '5464af81efbd39a09db61aa346cbea9416538cbd09edfa5a2930d92f0c9e4a65',\n",
       "  'prompt': 'What entertainment system is included in the vehicle?',\n",
       "  'response': 'The vehicle includes the Chevrolet Infotainment 3 Plus system with an 8-inch diagonal HD color touch-screen.',\n",
       "  'vector_distance': 0.0709647536278,\n",
       "  'inserted_at': 1760465207.27,\n",
       "  'updated_at': 1760465207.27,\n",
       "  'key': 'llmcache:5464af81efbd39a09db61aa346cbea9416538cbd09edfa5a2930d92f0c9e4a65'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T18:06:50.692570Z",
     "start_time": "2025-10-14T18:06:50.675988Z"
    }
   },
   "cell_type": "code",
   "source": "cache.check(\"Does the colorado drive on the water?\")",
   "id": "24a86603fa3c87eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
